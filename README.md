CS 5890 - Hidden Markov Models
================================

    Ross Nordstrom
    University of Colorado - Colorado Springs
    CS 5890 - Computational Linguistics

## Assignment

1. (By hand) Design an HMM for POS tagging on a provided dataset, including a drawing of the HMM structure.
2. Use supervised learning -- with a random 90% of the labeled dataset -- to obtain the probabilities needed for the HMM.
3. Using the learned paramter values, label the remaining 10% of the sentences using the Viterbi algo and the given POS tags.  Implement the Viterbi algo yourself.  Report precision (how many labels are correct).  Repeat #2 and #3 a few times and report average results.
4. _Extra Credit:_ Use expectation maximization to learn the parameters using the unlabeled dataset. Compute precision as done in #3.

## Dataset
Data was generated by Professor Kalita and the class.

## Usage
This project is intended to be used via the CLI.

```sh
npm install
npm link

rdnHmm --help
```

### Testing
```sh
npm install
npm test
```

### Results

Example results after 10,000 iterations on the "Full" Dataset

```sh
$ time rdnhmm -q -n 10000
Results from 10000 iterations of the HMM:

Sentence Precisions (freq by val): {
  "1": 187,
  "0.9": 846,
  "0.8": 2028,
  "0.7": 2674,
  "0.6": 2224,
  "0.5": 1323,
  "0.4": 551,
  "0.3": 141,
  "0.2": 26
}

Mean: 0.67,  Min: 0.20,  Max: 1.00

Tag Precisions (freq by val): {
  "1": 207,
  "0.97": 7,
  "0.96": 567,
  "0.95": 77,
  "0.93": 170,
  "0.92": 679,
  "0.91": 338,
  "0.9": 1079,
  "0.85": 1642,
  "0.8": 1481,
  "0.75": 1361,
  "0.7": 916,
  "0.65": 665,
  "0.6": 527,
  "0.5": 221,
  "0.4": 53,
  "0.3": 9,
  "0.2": 1
}

Mean: 0.80,  Min: 0.22,  Max: 1.00

real	0m33.772s
user	0m33.595s
sys 	0m0.240s
```
